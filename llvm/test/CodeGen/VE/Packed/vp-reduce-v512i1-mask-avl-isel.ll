; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -O0 --march=ve -mattr=+packed %s -o=/dev/stdout | FileCheck %s

define fastcc i1 @test_reduce_and(<512 x i1> %v, <512 x i1> %m, i32 %n) {
; CHECK-LABEL: test_reduce_and:
; CHECK:       # %bb.0:
; CHECK-NEXT:    adds.l %s11, -64, %s11
; CHECK-NEXT:    brge.l.t %s11, %s8, .LBB0_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    ld %s61, 24(, %s14)
; CHECK-NEXT:    or %s62, 0, %s0
; CHECK-NEXT:    lea %s63, 315
; CHECK-NEXT:    shm.l %s63, (%s61)
; CHECK-NEXT:    shm.l %s8, 8(%s61)
; CHECK-NEXT:    shm.l %s11, 16(%s61)
; CHECK-NEXT:    monc
; CHECK-NEXT:    or %s0, 0, %s62
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    svm %s16, %vm5, 0
; CHECK-NEXT:    st %s16, (, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 1
; CHECK-NEXT:    st %s16, 8(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 2
; CHECK-NEXT:    st %s16, 16(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 3
; CHECK-NEXT:    st %s16, 24(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 0
; CHECK-NEXT:    st %s16, 32(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 1
; CHECK-NEXT:    st %s16, 40(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 2
; CHECK-NEXT:    st %s16, 48(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 3
; CHECK-NEXT:    st %s16, 56(, %s11) # 64-byte Folded Spill
; CHECK-NEXT:    andm %vm4, %vm0, %vm2
; CHECK-NEXT:    andm %vm5, %vm0, %vm3
; CHECK-NEXT:    # implicit-def: $vmp1
; CHECK-NEXT:    ld %s16, (, %s11)
; CHECK-NEXT:    lvm %vm3, 0, %s16
; CHECK-NEXT:    ld %s16, 8(, %s11)
; CHECK-NEXT:    lvm %vm3, 1, %s16
; CHECK-NEXT:    ld %s16, 16(, %s11)
; CHECK-NEXT:    lvm %vm3, 2, %s16
; CHECK-NEXT:    ld %s16, 24(, %s11)
; CHECK-NEXT:    lvm %vm3, 3, %s16
; CHECK-NEXT:    ld %s16, 32(, %s11)
; CHECK-NEXT:    lvm %vm2, 0, %s16
; CHECK-NEXT:    ld %s16, 40(, %s11)
; CHECK-NEXT:    lvm %vm2, 1, %s16
; CHECK-NEXT:    ld %s16, 48(, %s11)
; CHECK-NEXT:    lvm %vm2, 2, %s16
; CHECK-NEXT:    ld %s16, 56(, %s11) # 64-byte Folded Reload
; CHECK-NEXT:    lvm %vm2, 3, %s16
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    andm %vm6, %vm0, %vm3
; CHECK-NEXT:    andm %vm1, %vm0, %vm5
; CHECK-NEXT:    andm %vm1, %vm1, %vm6
; CHECK-NEXT:    # implicit-def: $sx1
; CHECK-NEXT:    or %s1, 0, %s0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    or %s3, 0, %s1
; CHECK-NEXT:    lvl %s3
; CHECK-NEXT:    pcvm %s1, %vm1
; CHECK-NEXT:    # implicit-def: $sx2
; CHECK-NEXT:    or %s2, 0, %s3
; CHECK-NEXT:    xor %s1, %s1, %s2
; CHECK-NEXT:    # kill: def $vm2 killed $vm2 killed $vmp1
; CHECK-NEXT:    andm %vm1, %vm0, %vm4
; CHECK-NEXT:    andm %vm1, %vm1, %vm2
; CHECK-NEXT:    adds.w.sx %s2, 1, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s2
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    or %s3, 0, %s0
; CHECK-NEXT:    lvl %s3
; CHECK-NEXT:    pcvm %s0, %vm1
; CHECK-NEXT:    # implicit-def: $sx2
; CHECK-NEXT:    or %s2, 0, %s3
; CHECK-NEXT:    xor %s0, %s0, %s2
; CHECK-NEXT:    or %s0, %s0, %s1
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    cmps.w.zx %s1, %s0, (0)1
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s1
; CHECK-NEXT:    ldz %s0, %s0
; CHECK-NEXT:    srl %s0, %s0, 6
; CHECK-NEXT:    or %s1, 0, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s1
; CHECK-NEXT:    lea %s11, 64(, %s11)
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call i1 @llvm.vp.reduce.and.v512i1(<512 x i1> %v, <512 x i1> %m, i32 %n)
  ret i1 %r
}

define fastcc i1 @test_reduce_or(<512 x i1> %v, <512 x i1> %m, i32 %n) {
; CHECK-LABEL: test_reduce_or:
; CHECK:       # %bb.0:
; CHECK-NEXT:    adds.l %s11, -64, %s11
; CHECK-NEXT:    brge.l.t %s11, %s8, .LBB1_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    ld %s61, 24(, %s14)
; CHECK-NEXT:    or %s62, 0, %s0
; CHECK-NEXT:    lea %s63, 315
; CHECK-NEXT:    shm.l %s63, (%s61)
; CHECK-NEXT:    shm.l %s8, 8(%s61)
; CHECK-NEXT:    shm.l %s11, 16(%s61)
; CHECK-NEXT:    monc
; CHECK-NEXT:    or %s0, 0, %s62
; CHECK-NEXT:  .LBB1_2:
; CHECK-NEXT:    svm %s16, %vm5, 0
; CHECK-NEXT:    st %s16, (, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 1
; CHECK-NEXT:    st %s16, 8(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 2
; CHECK-NEXT:    st %s16, 16(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 3
; CHECK-NEXT:    st %s16, 24(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 0
; CHECK-NEXT:    st %s16, 32(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 1
; CHECK-NEXT:    st %s16, 40(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 2
; CHECK-NEXT:    st %s16, 48(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 3
; CHECK-NEXT:    st %s16, 56(, %s11) # 64-byte Folded Spill
; CHECK-NEXT:    andm %vm4, %vm0, %vm2
; CHECK-NEXT:    andm %vm5, %vm0, %vm3
; CHECK-NEXT:    # implicit-def: $vmp1
; CHECK-NEXT:    ld %s16, (, %s11)
; CHECK-NEXT:    lvm %vm3, 0, %s16
; CHECK-NEXT:    ld %s16, 8(, %s11)
; CHECK-NEXT:    lvm %vm3, 1, %s16
; CHECK-NEXT:    ld %s16, 16(, %s11)
; CHECK-NEXT:    lvm %vm3, 2, %s16
; CHECK-NEXT:    ld %s16, 24(, %s11)
; CHECK-NEXT:    lvm %vm3, 3, %s16
; CHECK-NEXT:    ld %s16, 32(, %s11)
; CHECK-NEXT:    lvm %vm2, 0, %s16
; CHECK-NEXT:    ld %s16, 40(, %s11)
; CHECK-NEXT:    lvm %vm2, 1, %s16
; CHECK-NEXT:    ld %s16, 48(, %s11)
; CHECK-NEXT:    lvm %vm2, 2, %s16
; CHECK-NEXT:    ld %s16, 56(, %s11) # 64-byte Folded Reload
; CHECK-NEXT:    lvm %vm2, 3, %s16
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    andm %vm6, %vm0, %vm3
; CHECK-NEXT:    andm %vm1, %vm0, %vm5
; CHECK-NEXT:    andm %vm1, %vm1, %vm6
; CHECK-NEXT:    # implicit-def: $sx1
; CHECK-NEXT:    or %s1, 0, %s0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    pcvm %s1, %vm1
; CHECK-NEXT:    # kill: def $vm2 killed $vm2 killed $vmp1
; CHECK-NEXT:    andm %vm1, %vm0, %vm4
; CHECK-NEXT:    andm %vm1, %vm1, %vm2
; CHECK-NEXT:    adds.w.sx %s2, 1, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s2
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    pcvm %s0, %vm1
; CHECK-NEXT:    or %s0, %s0, %s1
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    cmpu.w %s1, 0, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s1
; CHECK-NEXT:    srl %s0, %s0, 31
; CHECK-NEXT:    or %s1, 0, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s1
; CHECK-NEXT:    lea %s11, 64(, %s11)
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call i1 @llvm.vp.reduce.or.v512i1(<512 x i1> %v, <512 x i1> %m, i32 %n)
  ret i1 %r
}

define fastcc i1 @test_reduce_xor(<512 x i1> %v, <512 x i1> %m, i32 %n) {
; CHECK-LABEL: test_reduce_xor:
; CHECK:       # %bb.0:
; CHECK-NEXT:    adds.l %s11, -64, %s11
; CHECK-NEXT:    brge.l.t %s11, %s8, .LBB2_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    ld %s61, 24(, %s14)
; CHECK-NEXT:    or %s62, 0, %s0
; CHECK-NEXT:    lea %s63, 315
; CHECK-NEXT:    shm.l %s63, (%s61)
; CHECK-NEXT:    shm.l %s8, 8(%s61)
; CHECK-NEXT:    shm.l %s11, 16(%s61)
; CHECK-NEXT:    monc
; CHECK-NEXT:    or %s0, 0, %s62
; CHECK-NEXT:  .LBB2_2:
; CHECK-NEXT:    svm %s16, %vm5, 0
; CHECK-NEXT:    st %s16, (, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 1
; CHECK-NEXT:    st %s16, 8(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 2
; CHECK-NEXT:    st %s16, 16(, %s11)
; CHECK-NEXT:    svm %s16, %vm5, 3
; CHECK-NEXT:    st %s16, 24(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 0
; CHECK-NEXT:    st %s16, 32(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 1
; CHECK-NEXT:    st %s16, 40(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 2
; CHECK-NEXT:    st %s16, 48(, %s11)
; CHECK-NEXT:    svm %s16, %vm4, 3
; CHECK-NEXT:    st %s16, 56(, %s11) # 64-byte Folded Spill
; CHECK-NEXT:    andm %vm4, %vm0, %vm2
; CHECK-NEXT:    andm %vm5, %vm0, %vm3
; CHECK-NEXT:    # implicit-def: $vmp1
; CHECK-NEXT:    ld %s16, (, %s11)
; CHECK-NEXT:    lvm %vm3, 0, %s16
; CHECK-NEXT:    ld %s16, 8(, %s11)
; CHECK-NEXT:    lvm %vm3, 1, %s16
; CHECK-NEXT:    ld %s16, 16(, %s11)
; CHECK-NEXT:    lvm %vm3, 2, %s16
; CHECK-NEXT:    ld %s16, 24(, %s11)
; CHECK-NEXT:    lvm %vm3, 3, %s16
; CHECK-NEXT:    ld %s16, 32(, %s11)
; CHECK-NEXT:    lvm %vm2, 0, %s16
; CHECK-NEXT:    ld %s16, 40(, %s11)
; CHECK-NEXT:    lvm %vm2, 1, %s16
; CHECK-NEXT:    ld %s16, 48(, %s11)
; CHECK-NEXT:    lvm %vm2, 2, %s16
; CHECK-NEXT:    ld %s16, 56(, %s11) # 64-byte Folded Reload
; CHECK-NEXT:    lvm %vm2, 3, %s16
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    andm %vm6, %vm0, %vm3
; CHECK-NEXT:    andm %vm1, %vm0, %vm5
; CHECK-NEXT:    andm %vm1, %vm1, %vm6
; CHECK-NEXT:    # implicit-def: $sx1
; CHECK-NEXT:    or %s1, 0, %s0
; CHECK-NEXT:    and %s1, %s1, (32)0
; CHECK-NEXT:    srl %s1, %s1, 1
; CHECK-NEXT:    # kill: def $sw1 killed $sw1 killed $sx1
; CHECK-NEXT:    lvl %s1
; CHECK-NEXT:    pcvm %s1, %vm1
; CHECK-NEXT:    # kill: def $vm2 killed $vm2 killed $vmp1
; CHECK-NEXT:    andm %vm1, %vm0, %vm4
; CHECK-NEXT:    andm %vm1, %vm1, %vm2
; CHECK-NEXT:    adds.w.sx %s2, 1, %s0
; CHECK-NEXT:    # implicit-def: $sx0
; CHECK-NEXT:    or %s0, 0, %s2
; CHECK-NEXT:    and %s0, %s0, (32)0
; CHECK-NEXT:    srl %s0, %s0, 1
; CHECK-NEXT:    # kill: def $sw0 killed $sw0 killed $sx0
; CHECK-NEXT:    lvl %s0
; CHECK-NEXT:    pcvm %s0, %vm1
; CHECK-NEXT:    xor %s0, %s0, %s1
; CHECK-NEXT:    and %s0, 1, %s0
; CHECK-NEXT:    lea %s11, 64(, %s11)
; CHECK-NEXT:    b.l.t (, %s10)
  %r = call i1 @llvm.vp.reduce.xor.v512i1(<512 x i1> %v, <512 x i1> %m, i32 %n)
  ret i1 %r
}

declare i1 @llvm.vp.reduce.and.v512i1(<512 x i1>, <512 x i1> mask, i32 vlen)
declare i1 @llvm.vp.reduce.xor.v512i1(<512 x i1>, <512 x i1> mask, i32 vlen)
declare i1 @llvm.vp.reduce.or.v512i1(<512 x i1>, <512 x i1> mask, i32 vlen)
